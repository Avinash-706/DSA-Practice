1. Binary Array                       - Partition of QuickSort
2. Array with three values            -  Partition of QuickSort
3. Array of size n and small ranged values(100 - 200)        - Counting Sort | T.C. - O(n + k) | S.C. - O(k) Extra Space
4. Array of size n and range is of size n² and n³ or closer  - Radix Sort | T.C. - O(n) | S.C. - O(n)  Extra Space
5. Array of uniformly distributed data over a range          - Bucket Sort
6. When memory writes are costly      - Selection Sort | Cycle Sort
7. When adjacent swaps are allowed    - Bubble Sort | CockTail Sort ✅ (traverse from both the sides)
8. When array size is small           - Selection Sort | Insertion Sort ✅
9. When avaible extra memory is less  - Shell Sort - O(n (log n)²)


General Purpose Sorting Algorithms
1. Merge Sort - Stable Sort Algorithm | Works Greatly For LinkedList | work great for tape drives | Follows Divide and Conquer | Parallel Sorting
2. Heap Sort  - In-Place Sorting | Not Stable | Good For Priority Queues | Suitable When Constant Space is Required
3. Quick Sort - Fastest Among Above | Follows Divide and Conquer | Parallel Sorting | In-Place But Not Stable | Performs Well On Average Cases

Hybrid Algorithms (Used in Libraries) 
1. Tim Sort   - Mix of Insertion & Merge Sort | Uses Merge Sort for General Input | Uses Insertion Sort when - Inputs is Small OR Get Small in Recursive Calls   
2. Intro Sort - Mix of Quick, Heap & Insertion Sort | Uses Quick Sort for General Input | When Number of Recursive Calls Crosses (log n) -> Heap Sort | When Array Size is Small Insertion Sort


Sort in C++        -> Intro Sort
Worst Case &          Generally Uses QuickSort 
Average Case          If QuickSort is doing unfair partitioning(if more than n log n time) - switches to Heap Sort 
(n log n)             When Size of the Array is very small - switches to Insertion Sort         


Stable Sort in C++ -> Merge Sort ✅ | Tim Sort |
                      std::stable_sort (Uses Intro Sort and ensures stability by switching to Merge Sort when needed) 
                      Useful when order of equal elements needs to be preserved

     DEFINATION    -> A sorting algorithm is considered stable if it maintains the relative order of records with equal keys. 
                      In simpler terms, if two elements have the same value, their original order in the input array will be preserved in the output array.


 STABLE  SORT EXAMPLE   -> Bubble   , Insertion, Merge Sort
UNSTABLE SORT EXAMPLE   -> Selection,     Quick, Heap  Sort


BUBBLE SORT        -> Comparison-based algorithm | Best Case - O(n), Average & Worst Case - O(n²)
                   -> Optimized Version Breaks Early if Array is Already Sorted ✅
                   -> Stable Algorithm ✅
                   -> In-Place Algorithm - DOESN'T REQUIRE any Extra Space for Sorting ✅


SELECTION SORT     -> Comparision based algorithm | All Case - O(n²)
                   -> Does Less Memory Write Compare to Other Algorithms ✅
                   -> Exception: CYCLE SORT - more optimal than Selection Sort in terms of Memory Write
                   -> Heap Sort idea is Based on Selection Sort only ✅
                   -> Not Stable Algorithm ❌
                   -> In-Place Algorithm - DON'T REQUIRE any Extra Space for Sorting ✅


INSERTION SORT     -> Comparison-based algorithm | Best Case - O(n), when array is already sorted | Worst Case & Average Case - O(n²), when array is reverse sorted
                   -> Used in pratice for Small Array (Tim Sort and Intro Sort)
                   -> Stable Algorithm ✅
                   -> In-Place Algorithm ✅


MERGE SORT         -> Comparison-based algorithm | All Cases - O(n log n) | 
                   -> Preferred for Sorting Linked Lists and External Sorting ✅
                   -> Stable Algorithm ✅
                   -> Not In-Place Algorithm - REQUIRES Extra Space for Sorting ❌


CYCLE SORT         -> Comparison-based algorithm | Best Case - O(n²), Average & Worst Case - O(n²)
                   -> Does MINIMUM MEMORY WRITES so, Works Best When Memory Writes Are Costly (e.g., Flash Memory, EEPROM) ✅
                   -> NOT a Stable Algorithm ❌
                   -> In-Place Algorithm - DOESN'T REQUIRE any Extra Space for Sorting ✅
                   -> Finds the correct position of every element in a single cycle ✅
                   -> Useful for solving problems like "Find Minimum Swaps Required To Sort an Array ?" ✅
                   -> Can be used for Distinct Elements Sorting in O(n) (Ideal Case) ✅
                   -> Unstable Behavior: Same elements might be displaced relative to each other ❌


QUICK SORT         -> Comparison-based algorithm | Best case & Average Case - O(n log n) ✅ | Worst Case - O(n²) ❌  
                   -> NOT a Stable Algorithm ❌  
                   -> Despite O(n²), it is considered faster due to the following reasons:  
                   -> In-Place Algorithm ✅ | But Needs Recursion Call Stack ❌  
                   -> Cache Friendly Algorithm ✅  
                   -> Tail Recursive ✅  
                   -> Partition is key Function (Naive, Lomuto, Hoare) ✅  
                   -> Space Complexity: O(1) for in-place sorting ✅ | but RECURSION STACK : O(log n)  (best/average case) ✅ | O(n) (worst case) ❌  


HEAP SORT          -> Comparison-based algorithm | Best, Average & Worst Case - O(n log n)  
                   -> Inspired by Selection Sort – Instead of repeatedly finding the minimum/maximum element, it efficiently extracts the root from a heap ✅  
                   -> Heapify Process is Key – Uses Binary Heap (Max-Heap or Min-Heap) to build the sorted order ✅  
                   -> Works in Two Steps:  
                      1️⃣ Build a Heap (Heapify Process) – Convert the array into a Max-Heap (for Ascending Sort) or Min-Heap (for Descending Sort)  
                      2️⃣ Extract Elements One by One – Swap the root with the last element and re-heapify until sorted  
                   -> NOT a Stable Algorithm ❌  
                   -> In-Place Algorithm – DOESN'T REQUIRE any Extra Space for Sorting ✅  
                   -> Preferred for large datasets & widely used in Priority Queues ✅  
                   -> Heap Operations: Build Heap (O(n)) + Extract Elements One by One (O(n log n)) ✅  
                   -> Heap Sort is SLOWER THAN QUICK SORT in practice due to more cache misses ❌  



COUNTING SORT      -> NON-Comparison-based algorithm | Best, Average & Worst Case - O(n + k), where ‘k’ is the range of input values  
                   -> Performs Best When Range of Numbers is Small Compared to 'n' ✅  
                   -> NOT Suitable for Large Range of Numbers due to High Space Usage ❌  
                   -> Stable Algorithm ✅  
                   -> NOT an In-Place Algorithm - therefore, REQUIRES Extra Space ❌  


RADIX SORT         -> NON-Comparison-based algorithm | Best, Average & Worst Case - O(nk), where ‘k’ is the number of digits in the max number  
                   -> Works Best When 'k' is Small Compared to 'n' ✅  
                   -> Uses Counting Sort as a Subroutine ✅  
                   -> Stable Algorithm ✅  
                   -> NOT an In-Place Algorithm - REQUIRES Extra Space ❌  


BUCKET SORT        -> NON-Comparison-based algorithm | Best Case - O(n + k) | Worst Case - O(n²)  
                   -> Distributes Elements into Buckets & Sorts Individually (Often using Insertion Sort) ✅  
                   -> Performs Best When Data is Uniformly Distributed ✅  
                   -> Stable Algorithm (Depends on Subroutine Sorting Algorithm) ✅  
                   -> NOT an In-Place Algorithm - REQUIRES Extra Space ❌  
        

LOMUTO PARTITION   -> Picks the last element as a pivot ✅  
                   -> Always places the pivot at its correct position ✅  
                   -> Requires a single scan for partitioning ✅  
                   -> Performs poorly on sorted input (Worst Case: O(n²)) ❌  
                   -> Requires more swaps compared to Hoare ❌  
                   -> Suitable for teaching purposes due to its simplicity ✅  
                   -> Space Complexity: O(1) for in-place partitioning ✅ | O(n) recursion stack in worst case ❌ | O(log n) in best/average case ✅  


HOARE PARTITION    -> Picks the first element as a pivot ✅  
                   -> Partitions the array more efficiently with fewer swaps ✅  
                   -> Leaves the pivot at an arbitrary position ❌  
                   -> Works well even with sorted input, reducing worst-case chances ✅  
                   -> More efficient than Lomuto due to fewer swaps ✅  
                   -> Requires two scans for partitioning ❌  
                   -> **Space Complexity:** O(1) for in-place partitioning ✅ | O(log n) recursion stack in best/average case ✅ | O(n) recursion stack in worst case ❌ (less frequent than Lomuto)  


---------------------------------------------------------------------------------------------------------------------
| Feature                 |   Lomuto Partitioning                      |   Hoare Partitioning                       |
---------------------------------------------------------------------------------------------------------------------
| Pivot Selection         | Last element                               | First element                              |
| Partitioning Process    | Elements ≤ pivot on left, > pivot on right | Elements < pivot on left, > pivot on right |
| Number of Comparisons   | More comparisons                           | Fewer comparisons                          |
| Swaps                   | More swaps                                 | Fewer swaps                                |
| Best & Avg Time Comp.   | O(n log n) ✅                              | O(n log n) ✅                             |
| Worst Time Complexity   | O(n²) ❌                                   | O(n²) ❌                                  |
| Stability               | Not stable ❌                              | Not stable ❌                             |
| Space Complexity        | O(n) worst case ❌                         | O(log n) avg case ✅                      |
| Efficiency in Practice  | Less efficient ❌                          | More efficient ✅                         |
| Recursion Depth         | O(n) worst case ❌                         | O(log n) avg case ✅                      |
| In-Place Sorting        | Yes ✅                                     | Yes ✅                                    |
| Tail Recursion          | Yes ✅                                     | Yes ✅                                    |
| Cache Friendly          | No ❌                                      | Yes ✅                                    |
| Preferred Use Case      | Simpler but less efficient                 | More efficient and preferred               |
---------------------------------------------------------------------------------------------------------------------
