 Hashing is a technique primarily used to implement dictionaries (key-value pairs) and sets (unique keys)
 It offers O(1) average time complexity for search, insert, and delete operations, making it highly efficient
 compared to arrays and self-balancing binary search trees


OPERATIONS:

 Search ->  Quickly find a value using a key (e.g., finding details using a phone number)
 Insert ->  Add a key-value pair; if the key exists, the value is updated
 Delete ->  Remove a key and its corresponding value


COMPARISON WITH OTHER DATA STRUCTURES:

                     Arrays                          ->  Sorted arrays provide O(log n) search but slow O(n) insert/delete
                                                         Unsorted arrays allow O(1) insert/delete but slow O(n) search
    Binary Search Trees (BSTs, AVL, Red-Black Trees) ->  Provide O(log n) search, insert, and delete, making them slower than hashing for these operations


LIMITATIONS OF HASHING:

 -> Cannot find closest smaller/larger key efficiently (O(n) time)
 -> Does not maintain sorted order of keys
 -> Inefficient for range queries and prefix searching (Trie is better for prefix searches)
 -> Thus, hashing is an optimal choice when only search, insert, and delete operations are needed without concerns for ordering


APPLICATIONS OF HASHING

1. Dictionary                                 ->   Implementation Used for storing and searching words efficiently in constant time
2. Database Indexing                          ->   Hashing is used for primary and secondary indexing to speed up record retrieval
3. Cryptography                               ->   Passwords are stored as hashed values for security. Authentication is done by comparing stored hashes
4. Caching                                    ->   Used in browser caches, where URLs are keys, and stored data is values
5. Symbol Tables in Compilers                 ->   Helps in quick lookup of variables and addresses in programming languages
6. Networking (Routers & IP Mapping)          ->   Routers use hashing to store MAC addresses and IP mappings for fast retrieval
7. Associative Arrays & Database Operations   ->   Many programming languages use associative arrays (hash tables) for database queries and data storage
8. Other Use Cases                            ->   Hashing is widely used across all fields of computer science for efficient data handling and retrieval


DIRECT ADDRESS TABLE (DAT)

 Direct Address Table is a simple yet efficient technique used in hashing to 
 store and retrieve data in O(1) time complexity for fundamental operations
 
 FEATURES:
  -> Time Complexity: O(1) for Search, Insert, and Delete operations ✅
  -> Uses a Boolean array to mark the presence or absence of elements ✅
  -> Works well when keys are small and within a limited range (e.g., 0 to 999) ✅
  -> Random Access in O(1) time since we directly use the key as an index ✅
 
 LIMITATIONS:
  -> Cannot handle large key values (e.g., phone numbers with 10 digits) ❌
  -> Does not support floating-point numbers or strings as keys ❌
  -> Wastes memory when keys are sparse, as a large array must be allocated even if only a few keys are used ❌
 

HASH MAP (HASH TABLE)

 To overcome DAT's limitations, hashing is introduced, which enables efficient storage
 and retrieval of data even for large keys
 
 Instead of creating a massive array, we use a hash function to map large keys into a fixed-size array
 A hash function (h) takes an input key and returns an array index: h(KEY) = INDEX
 
 This allows storage and retrieval in O(1) average time complexity while significantly reducing memory usage
 
 ADVANTAGES of HASH MAP over DAT:
  -> Can handle large keys, floating-point numbers, and strings efficiently
  -> Requires less memory compared to DAT
  -> Works well for dynamic datasets where keys are not sequential
  -> Supports collision handling techniques like chaining and open addressing for better performance
  
 CHALLENGES with HASHING:
  -> Collisions:                                     Two different keys may produce the same index, requiring additional handling
  -> Performance depends on hash function quality:   A poor hash function may cause clustering, reducing efficiency


KEY CONCEPTS IN HASHING:
I.   HASH FUNCTION         -> Converts a key (e.g., phone number, string) into an index within a fixed-size array
II.  EFFICIENT OPERATIONS  -> Insert, Search, and Delete in O(1) average time complexity
III. COLLISION             -> Occurs when two different keys map to the same index in the hash table


I.   HASH FUNCTION
 To overcome DAT’s limitations, a hash function is introduced to convert large keys 
 into small indices that fit in a hash table

 Instead of using keys as direct indices, hash functions generate an index from 
 large values (e.g., phone numbers, names, employee IDs)
 This allows storage of large key sets in a compact array, optimizing search, insert, and delete operations
 
 PROPERTIES OF A GOOD HASH FUNCTION
  Consistent Mapping       –>  The same key should always map to the same index
  Valid Index Generation   –>  Output should be between 0 to M-1 (M = hash table size)
  Fast Computation         –>  Should run in O(1) time for integers and O(length) time for strings
  Uniform Distribution     –>  Should distribute keys evenly across the hash table to reduce collisions
 
 TYPES OF HASH FUNCTIONS

  1. Modulo-Based Hashing → hash(key) = key % M
   ->      Simple and effective for integers
   ->      Works best when M is a prime number
  
  2. Multiplicative Hashing → Uses a constant multiplier to spread keys evenly
  3. String Hashing → Converts string characters into an index using ASCII values and a polynomial formula:
   ->      hash(str) = (char1 * x^0 + char2 * x^1 + char3 * x^2 + ...) % M
   ->      Choosing a prime x (e.g., 33, 37) helps avoid collisions
  
  4. Universal Hashing → Uses a random hash function from a set to ensure a good distribution


III. THE COLLISION PROBLEM

 Even with a well-designed hash function, collisions are unavoidable in most real-world scenarios

 UNDERSTANDING COLLISIONS:

  If you have a hash table of size 10 and try storing 10 phone numbers, there’s a high chance
  that two numbers will share the same index

  The BIRTHDAY PARADOX    ->   explains why collisions happen frequently:

   In a room of 23 people, there is a 50% chance that two share the same birthday
   With 70 people, this probability increases to 99.9%
   Similarly, in hashing, no matter how large your table is, collisions are inevitable
   when keys are not known in advance

  COLLISION HANDLING TECHNIQUES

   1. CHAINING (SEPARATE CHAINING)
    Each index in the hash table stores a linked list (or a dynamic data structure) to hold multiple values
    If multiple keys hash to the same index, they are stored in a chain
    Simple to implement ✅
    Handles an unlimited number of keys per index ✅
    Uses extra memory for pointers ❌
    May degrade to O(n) search time if chains are long ❌

   2. OPEN ADDRESSING
    Instead of using a linked list, all elements are stored within the hash table itself
    If a collision occurs, the algorithm finds another available slot using a predefined probe sequence

    TYPES OF OPEN ADDRESSING:
     1. Linear Probing – Search for the next available slot sequentially
     2. Quadratic Probing – Use a quadratic function to find the next slot
     3. Double Hashing – Use a secondary hash function to resolve collisions

    No extra memory for linked lists ✅
    Performance may degrade if the table is nearly full ❌
    Clustering: Elements may group together, reducing efficiency ❌

   3. PERFECT HASHING (SPECIAL CASE)

    If all keys are known in advance, a perfect hash function can be designed to ensure no collisions
    Used in dictionaries, compilers, and static datasets

 -> Collisions are unavoidable in most hashing scenarios
 -> Chaining is flexible but uses extra memory
 -> Open Addressing is memory-efficient but may suffer from clustering
 -> Perfect Hashing can eliminate collisions but requires prior knowledge of all keys
 -> Understanding these techniques helps in designing efficient hash-based data 
    structures like unordered_map (C++), HashMap (Java), and HashSet (Java)


1. CHAINING
 Hashing with chaining is a method to handle collisions in hash tables by maintaining a 
 linked list at each index where a collision occurs

 FEATURES:
  -> Uses an array of linked lists to store multiple keys at the same index  
  -> Each slot in the hash table points to the head of a linked list  
  -> Efficient for handling collisions in hash tables  
  -> Hash function determines the index where a key is inserted  
  
 
 EXAMPLE:
   GIVEN:
   Keys: 50, 700, 76, 85, 92, 73, 101  
   Hash function: key % 7   
   Hash table size: 7  

   HASH TABLE CONSTRUCTION:
   ----------------------------------------------------
   KEY   | COMPUTATION       | INDEX | PLACEMENT
   ----------------------------------------------------
   50    | 50 % 7  = 1      | 1     | Index 1
   700   | 700 % 7 = 0      | 0     | Index 0
   76    | 76 % 7  = 6      | 6     | Index 6
   85    | 85 % 7  = 1      | 1     | Collision → Append at Index 1
   92    | 92 % 7  = 1      | 1     | Append at Index 1
   73    | 73 % 7  = 3      | 3     | Index 3
   101   | 101 % 7 = 3      | 3     | Collision → Append at Index 3  
   ----------------------------------------------------
   
   FINAL HASH TABLE REPRESENTATION:
   ----------------------------------------------------
   INDEX | VALUES (Linked List)
   ---------------------------------
   0     | 700
   1     | 50 → 85 → 92
   2     |  
   3     | 73 → 101
   4     |  
   5     |  
   6     | 76

 PERFORMANCE ANALYSIS:
  Number of slots in hash table: m  
  Number of keys to insert: n  
  Load factor (α) = n/m  
  Average chain length = n/m

 TIME COMPLEXITY ANALYSIS:
  ----------------------------------------------------
  OPERATION | COMPLEXITY
  ---------------------------
  Search    | O(1 + α)
  Insert    | O(1 + α)
  Delete    | O(1 + α)
  If α is small, operations are close to O(1), making chaining efficient

 DATA STRUCTURES FOR CHAINING:
  STRUCTURE        | PROS ✅ / CONS ❌
  -----------------|-----------------------------------
  Linked List      | ✅ Simple, dynamic insertions & deletions  
                   | ❌ Search time O(n/m) in worst case  
  Balanced BST     | ✅ O(log(n/m)) search  
                   | ✅ Better with high collision load  
                   | ❌ More complex  
  Dynamic Arrays   | ✅ Faster access for search  
                   | ❌ Uses contiguous memory  
                   | ❌ Resizing is costly  
  Hybrid Structures| ✅ Hash table with self-balancing BSTs  

 CONCLUSION:
  -> Chaining effectively handles hash table collisions ✅
  -> Performance depends on the load factor (α)  
  -> Using self-balancing BSTs within chains improves performance for large datasets ✅
  -> Choosing an optimal hash function and a prime table size minimizes collisions ✅


2. OPEN ADDRESSING IN HASHING
 -> Open Addressing is a method for handling collisions in hash tables ✅
 -> Unlike chaining, it uses a "SINGLE ARRAY" and does not involve additional data structures
 -> REQUIREMENT: Hash table size (m) >= number of keys (n)
 
 ADVANTAGES
  CACHE-FRIENDLY   -> All elements are in a single array, reducing cache misses ✅
  NO EXTRA MEMORY  -> No need for pointers or linked lists like in chaining ✅

 METHODS OF OPEN ADDRESSING
  I.   LINEAR PROBING
  II.  QUADRATIC PROBING
  III. DOUBLE HASHING
  
 I. LINEAR PROBING
  If a collision occurs, search linearly for the next available slot
  Uses a hash function:  
        hash(key) = key % table_size

  NOTE : If index is occupied, check (index+1) % table_size, and continue

  Example (Table Size = 7)
  | Key | Hash (Key % 7) |          Inserted At            |
  |-----|----------------|---------------------------------|
  | 50  | 50 % 7 = 1     | Index 1                         |
  | 51  | 51 % 7 = 2     | Index 2                         |
  | 49  | 49 % 7 = 0     | Index 0                         |
  | 16  | 16 % 7 = 2     | Collision → Next Slot → Index 3 |
  | 56  | 56 % 7 = 0     | Collision → Next Slot → Index 4 |
  | 15  | 15 % 7 = 1     | Collision → Next Slot → Index 5 |
  | 19  | 19 % 7 = 5     | Collision → Next Slot → Index 6 |

  ==========================================================
  | Index | Key  |
  |-------|------|
  | 0     | 49   |
  | 1     | 50   |
  | 2     | 51   |
  | 3     | 16   |
  | 4     | 56   |
  | 5     | 15   |
  | 6     | 19   |
  ==========================================================

  SEARCHING IN OPEN ADDRESSING
   Compute hash index:  
        hash(key) = key % table_size
  
   If key is not found at computed index:
   1. CONTINUE LINEARLY until key is found or an empty slot is encountered
   2. IF TABLE IS FULL, traverse in a circular manner
   3. STOP IF:
     Key is found → Return `TRUE`
     Empty slot is found → Return `FALSE`
     Entire table is traversed → Return `FALSE`

   ---
  
  DELETION IN OPEN ADDRESSING
   Simply removing an element can BREAK SEARCH OPERATIONS
   SOLUTION  ->  Use a special marker (e.g., 'DELETED' or '-1') instead of making the slot empty
  
  STEPS FOR DELETION:
   1. Search for the key
   2. If found, mark it as `DELETED`
   3. During search, continue past `DELETED` markers until:
     Key is found (Return `True`)
     Empty slot is found (Return `False`)
     
   ---

  PROBLEMS IN LINEAR PROBING ❌ :
   -> Linear probing resolves collisions by checking the next available slot
   -> Issue: Leads to 'PRIMARY CLUSTERING', where multiple keys crowd around the same index ❌
   Example: 
    `16 % 7 = 2` → Slot 2 is occupied → Moves to Index 3
    `56 % 7 = 0` → Slot 0 is occupied → Moves to Index 4
    `15 % 7 = 1` → Slot 1 is occupied → Moves to Index 5
    `19 % 7 = 5` → Slot 5 is occupied → Moves to Index 6
    PROBLEM: As the table fills up, collisions become frequent, and search times increase ❌

   ---

 II. QUADRATIC PROBING
  Instead of moving to the next slot, quadratic probing resolves collisions using a quadratic function: ✅
  FORMULA: `Hash(Key) = (H + i²) % Table_Size`
   ,Where `i` starts from `1` and increases (`1², 2², 3², ...`) until an empty slot is found

  EXAMPLE OF QUADRATIC PROBING:
   Insert `16` at `16 % 7 = 2` → Occupied
   Check `2 + 1² = 3` → Empty → Insert at Index 3
   Insert `56` at `56 % 7 = 0` → Occupied
   Check `0 + 1² = 1` → Occupied
   Check `0 + 2² = 4` → Empty → Insert at Index 4

   ---

  PROBLEMS IN QUADRATIC PROBING:
   1. SECONDARY CLUSTERING:  
    Quadratic probing reduces PRIMARY CLUSTERING, but SECONDARY CLUSTERING still occurs ✅
    Keys that hash to the same initial index follow the same probe sequence ❌

   2. TABLE SIZE DEPENDENCY: 
    For quadratic probing to work efficiently, the table size should be PRIME ❌
    If the table size is not properly chosen, certain indices might never be reached ❌

   3. WASTED SPACE & UNREACHABLE SLOTS:  
    If the table gets too full, quadratic probing might FAIL TO FIND AN EMPTY SLOT ❌
    Some indexes may never be checked due to the quadratic growth pattern ❌

   ---


 III. DOUBLE HASHING NOTES

  Double Hashing uses 2 hash functions:
   -> H1(Key) = Key % m (PRIMARY hash function)  
   -> H2(Key) helps find the NEXT SLOT when a COLLISION occurs  
  Unlike QUADRATIC PROBING, double hashing does not require:  
   -> Doubling the table size
   -> ‘m’ being prime (if `m` and `H2(Key)` are relatively prime)  

  ---------------------------------------------
  Advantages Over Other Probing Methods
   AVOIDS CLUSTERING          -> No SECONDARY CLUSTERS like in quadratic probing ✅
   UNIFORM KEY DISTRIBUTION   -> Distributes keys BETTER than linear and quadratic probing ✅
   GUARANTEED FREE SLOT       -> If `m` and `H2(Key)` are RELATIVELY PRIME, it always finds a free slot ✅

  ---------------------------------------------
  EXAMPLE: DOUBLE HASHING IN ACTION
   Given: `m = 7`  
   Hash Functions:  
    -> **H1(Key) = Key % 7**  
    -> **H2(Key) = 6 - (Key % 6)**  

   Inserting Keys:
   | Key  | H1(Key)           | H2(Key)                     | Final Index (with Probing)            |
   |------|-------------------|-----------------------------|---------------------------------------|
   | 49   | 0                 | -                           | 0                                     |
   | 63   | 0 (**Collision**) | 6 - (63 % 6) = 3            | (0 + 1×3) % 7 = 3                     |
   | 56   | 0 (**Collision**) | 6 - (56 % 6) = 4            | (0 + 1×4) % 7 = 4                     |
   | 52   | 3                 | 6 - (52 % 6) = 2            | (3 + 1×2) % 7 = 5                     |
   | 54   | 5                 | 6 - (54 % 6) = 6            | (5 + 1×6) % 7 = 4 (**Collision**)     |
   |      |                   |                             | (5 + 2×6) % 7 = 3 (**Collision**)     |
   |      |                   |                             | (5 + 3×6) % 7 = 2 (**Inserted at 2**) |
   | 48   | 6                 | -                           | 6 (**Directly inserted**)             |

   ---------------------------------------------

  IMPORTANCE OF RELATIVELY PRIME H2(Key) and m
   Ensures double hashing probes all slots  
   If `m` and `H2(Key)` are NOT RELATIVELY PRIME, it may not cover all indices  
   -> Example:  
     -> H2(Key) = 6, m = 7
     -> Generates probe sequence: `6, 5, 4, 3, 2, 1` → Covers all slots

   ---------------------------------------------

  DOUBLE HASHING ALGORITHM
   1. If the **table is full**, return an **error**  
   2. Compute **initial index** using `H1(Key)`  
   3. If occupied, compute **H2(Key)**  
   4. Probe new index using:  
      -> `Index = (H1(Key) + i × H2(Key)) % m`  
   5. Repeat until an empty slot is found  

   ---------------------------------------------
  
  SEARCHING IN DOUBLE HASHING
   -> Searching a **key follows the same probe sequence** as insertion  
   -> Steps:  
     1. Compute H1(Key)  
     2. Check if the VALUE AT H1(KEY) MATCHES the searched key  
     3. If NOT FOUND, compute H2(Key) and check index:  
        ->  `Index = (H1(Key) + i × H2(Key)) % m`  
     4. REPEAT until the key is found or an empty slot is reached
   
   If the SEARCH REACHES AN EMPTY SLOT, the key is not present ✅ 

   ---------------------------------------------
   
   PERFORMANCE ANALYSIS OF SEARCHING
    SEARCHING PERFORMANCE DEPENDS ON THE LOAD FACTOR (α = n/m)  
      -> n = NUMBER OF ELEMENTS INSERTED  
      -> m = TABLE SIZE  

    -> AVERAGE CASE:  
     SUCCESSFUL SEARCH    → O(1) when load factor is LOW ✅ 
     UNSUCCESSFUL SEARCH  → O(1) when table is SPARSE ✅ 

    -> WORST CASE (WHEN LOAD FACTOR IS HIGH):  
     search takes O(n) time in the worst case ✅ 
     occurs when most slots are filled, requiring MULTIPLE PROBES ✅ 

    -> EXPECTED PROBES (FOR SUCCESSFUL SEARCH):  
     approximately 1 / (1 - α), when α < 0.5 ✅
     grows rapidly as α approaches 1 ✅

    -> COMPARISON WITH OTHER PROBING METHODS:  
     double hashing requires FEWER PROBES than linear or quadratic probing ✅
     ensures UNIFORM DISTRIBUTION, REDUCING PROBING SEQUENCE LENGTH ✅


  DELETION IN DOUBLE HASHING
   -> DELETION IN OPEN ADDRESSING is handled using a SPECIAL MARKER  
   -> DIRECTLY DELETING A KEY can break the SEARCH CHAIN
   -> SOLUTION:  
     ✅ Use a "deleted" MARKER instead of emptying the slot  
     ✅ CONTINUE PROBING while searching, ignoring "deleted" SLOTS

   -> DELETION STEPS:  
    1️. LOCATE the key using the SEARCH ALGORITHM  
    2️. REPLACE THE VALUE with a "deleted" MARKER  
    3️. Future insertions can reuse the "deleted" SLOT  

   ---------------------------------------------

  KEY TAKEAWAYS
   -> Uses 2 HASH FUNCTIONS for BETTER COLLISION RESOLUTION ✅
   -> No clustering issue like linear/quadratic probing ✅
   -> Works efficiently if `H2(Key)` and `m` are RELATIVELY PRIME ✅
   -> Searching and deletion follow the same probing sequence ✅
   =============================================



## Summary
| Method | Description | Advantages | Disadvantages |
|--------|------------|------------|---------------|
| **Linear Probing** | Search for the next available slot linearly | Simple to implement | Clustering issue (many elements in adjacent slots) |
| **Quadratic Probing** | Search using quadratic increments | Reduces clustering | May not use all slots efficiently |
| **Double Hashing** | Uses a second hash function for jumps | Best distribution of keys | Slightly complex to compute |

✔ **Best for small datasets**  
❌ **Chaining is better for large datasets with many collisions**  



