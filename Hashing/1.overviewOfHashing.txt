 Hashing is a technique primarily used to implement dictionaries (key-value pairs) and sets (unique keys)
 It offers O(1) average time complexity for search, insert, and delete operations, making it highly efficient
 compared to arrays and self-balancing binary search trees


OPERATIONS:

 Search ->  Quickly find a value using a key (e.g., finding details using a phone number)
 Insert ->  Add a key-value pair; if the key exists, the value is updated
 Delete ->  Remove a key and its corresponding value


COMPARISON WITH OTHER DATA STRUCTURES:

                     Arrays                          ->  Sorted arrays provide O(log n) search but slow O(n) insert/delete
                                                         Unsorted arrays allow O(1) insert/delete but slow O(n) search
    Binary Search Trees (BSTs, AVL, Red-Black Trees) ->  Provide O(log n) search, insert, and delete, making them slower than hashing for these operations


LIMITATIONS OF HASHING:

 -> Cannot find closest smaller/larger key efficiently (O(n) time)
 -> Does not maintain sorted order of keys
 -> Inefficient for range queries and prefix searching (Trie is better for prefix searches)
 -> Thus, hashing is an optimal choice when only search, insert, and delete operations are needed without concerns for ordering


APPLICATIONS OF HASHING

1. Dictionary                                 ->   Implementation Used for storing and searching words efficiently in constant time
2. Database Indexing                          ->   Hashing is used for primary and secondary indexing to speed up record retrieval
3. Cryptography                               ->   Passwords are stored as hashed values for security. Authentication is done by comparing stored hashes
4. Caching                                    ->   Used in browser caches, where URLs are keys, and stored data is values
5. Symbol Tables in Compilers                 ->   Helps in quick lookup of variables and addresses in programming languages
6. Networking (Routers & IP Mapping)          ->   Routers use hashing to store MAC addresses and IP mappings for fast retrieval
7. Associative Arrays & Database Operations   ->   Many programming languages use associative arrays (hash tables) for database queries and data storage
8. Other Use Cases                            ->   Hashing is widely used across all fields of computer science for efficient data handling and retrieval


DIRECT ADDRESS TABLE (DAT)

 Direct Address Table is a simple yet efficient technique used in hashing to 
 store and retrieve data in O(1) time complexity for fundamental operations
 
 FEATURES:
  -> Time Complexity: O(1) for Search, Insert, and Delete operations âœ…
  -> Uses a Boolean array to mark the presence or absence of elements âœ…
  -> Works well when keys are small and within a limited range (e.g., 0 to 999) âœ…
  -> Random Access in O(1) time since we directly use the key as an index âœ…
 
 LIMITATIONS:
  -> Cannot handle large key values (e.g., phone numbers with 10 digits) âŒ
  -> Does not support floating-point numbers or strings as keys âŒ
  -> Wastes memory when keys are sparse, as a large array must be allocated even if only a few keys are used âŒ
 

HASH MAP (HASH TABLE)

 To overcome DAT's limitations, hashing is introduced, which enables efficient storage
 and retrieval of data even for large keys
 
 Instead of creating a massive array, we use a hash function to map large keys into a fixed-size array
 A hash function (h) takes an input key and returns an array index: h(KEY) = INDEX
 
 This allows storage and retrieval in O(1) average time complexity while significantly reducing memory usage
 
 ADVANTAGES of HASH MAP over DAT:
  -> Can handle large keys, floating-point numbers, and strings efficiently
  -> Requires less memory compared to DAT
  -> Works well for dynamic datasets where keys are not sequential
  -> Supports collision handling techniques like chaining and open addressing for better performance
  
 CHALLENGES with HASHING:
  -> Collisions:                                     Two different keys may produce the same index, requiring additional handling
  -> Performance depends on hash function quality:   A poor hash function may cause clustering, reducing efficiency


KEY CONCEPTS IN HASHING:
I.   HASH FUNCTION         -> Converts a key (e.g., phone number, string) into an index within a fixed-size array
II.  EFFICIENT OPERATIONS  -> Insert, Search, and Delete in O(1) average time complexity
III. COLLISION             -> Occurs when two different keys map to the same index in the hash table


I.   HASH FUNCTION
 To overcome DATâ€™s limitations, a hash function is introduced to convert large keys 
 into small indices that fit in a hash table

 Instead of using keys as direct indices, hash functions generate an index from 
 large values (e.g., phone numbers, names, employee IDs)
 This allows storage of large key sets in a compact array, optimizing search, insert, and delete operations
 
 PROPERTIES OF A GOOD HASH FUNCTION
  Consistent Mapping       â€“>  The same key should always map to the same index
  Valid Index Generation   â€“>  Output should be between 0 to M-1 (M = hash table size)
  Fast Computation         â€“>  Should run in O(1) time for integers and O(length) time for strings
  Uniform Distribution     â€“>  Should distribute keys evenly across the hash table to reduce collisions
 
 TYPES OF HASH FUNCTIONS

  1. Modulo-Based Hashing â†’ hash(key) = key % M
   ->      Simple and effective for integers
   ->      Works best when M is a prime number
  
  2. Multiplicative Hashing â†’ Uses a constant multiplier to spread keys evenly
  3. String Hashing â†’ Converts string characters into an index using ASCII values and a polynomial formula:
   ->      hash(str) = (char1 * x^0 + char2 * x^1 + char3 * x^2 + ...) % M
   ->      Choosing a prime x (e.g., 33, 37) helps avoid collisions
  
  4. Universal Hashing â†’ Uses a random hash function from a set to ensure a good distribution


III. THE COLLISION PROBLEM

 Even with a well-designed hash function, collisions are unavoidable in most real-world scenarios

 UNDERSTANDING COLLISIONS:

  If you have a hash table of size 10 and try storing 10 phone numbers, thereâ€™s a high chance
  that two numbers will share the same index

  The BIRTHDAY PARADOX    ->   explains why collisions happen frequently:

   In a room of 23 people, there is a 50% chance that two share the same birthday
   With 70 people, this probability increases to 99.9%
   Similarly, in hashing, no matter how large your table is, collisions are inevitable
   when keys are not known in advance

  COLLISION HANDLING TECHNIQUES

   1. CHAINING (SEPARATE CHAINING)
    Each index in the hash table stores a linked list (or a dynamic data structure) to hold multiple values
    If multiple keys hash to the same index, they are stored in a chain
    Simple to implement âœ…
    Handles an unlimited number of keys per index âœ…
    Uses extra memory for pointers âŒ
    May degrade to O(n) search time if chains are long âŒ

   2. OPEN ADDRESSING
    Instead of using a linked list, all elements are stored within the hash table itself
    If a collision occurs, the algorithm finds another available slot using a predefined probe sequence

    TYPES OF OPEN ADDRESSING:
     1. Linear Probing â€“ Search for the next available slot sequentially
     2. Quadratic Probing â€“ Use a quadratic function to find the next slot
     3. Double Hashing â€“ Use a secondary hash function to resolve collisions

    No extra memory for linked lists âœ…
    Performance may degrade if the table is nearly full âŒ
    Clustering: Elements may group together, reducing efficiency âŒ

   3. PERFECT HASHING (SPECIAL CASE)

    If all keys are known in advance, a perfect hash function can be designed to ensure no collisions
    Used in dictionaries, compilers, and static datasets

 -> Collisions are unavoidable in most hashing scenarios
 -> Chaining is flexible but uses extra memory
 -> Open Addressing is memory-efficient but may suffer from clustering
 -> Perfect Hashing can eliminate collisions but requires prior knowledge of all keys
 -> Understanding these techniques helps in designing efficient hash-based data 
    structures like unordered_map (C++), HashMap (Java), and HashSet (Java)


1. CHAINING
 Hashing with chaining is a method to handle collisions in hash tables by maintaining a 
 linked list at each index where a collision occurs.

 FEATURES:
  -> Uses an array of linked lists to store multiple keys at the same index  
  -> Each slot in the hash table points to the head of a linked list  
  -> Efficient for handling collisions in hash tables  
  -> Hash function determines the index where a key is inserted  
  
 
 EXAMPLE:
   GIVEN:
   Keys: 50, 700, 76, 85, 92, 73, 101  
   Hash function: key % 7   
   Hash table size: 7  

   HASH TABLE CONSTRUCTION:
   ----------------------------------------------------
   KEY   | COMPUTATION       | INDEX | PLACEMENT
   ----------------------------------------------------
   50    | 50 % 7  = 1      | 1     | Index 1
   700   | 700 % 7 = 0      | 0     | Index 0
   76    | 76 % 7  = 6      | 6     | Index 6
   85    | 85 % 7  = 1      | 1     | Collision â†’ Append at Index 1
   92    | 92 % 7  = 1      | 1     | Append at Index 1
   73    | 73 % 7  = 3      | 3     | Index 3
   101   | 101 % 7 = 3      | 3     | Collision â†’ Append at Index 3  
   ----------------------------------------------------
   
   FINAL HASH TABLE REPRESENTATION:
   ----------------------------------------------------
   INDEX | VALUES (Linked List)
   ---------------------------------
   0     | 700
   1     | 50 â†’ 85 â†’ 92
   2     |  
   3     | 73 â†’ 101
   4     |  
   5     |  
   6     | 76

 PERFORMANCE ANALYSIS:
  Number of slots in hash table: m  
  Number of keys to insert: n  
  Load factor (Î±) = n/m  
  Average chain length = n/m

 TIME COMPLEXITY ANALYSIS:
  ----------------------------------------------------
  OPERATION | COMPLEXITY
  ---------------------------
  Search    | O(1 + Î±)
  Insert    | O(1 + Î±)
  Delete    | O(1 + Î±)
  If Î± is small, operations are close to O(1), making chaining efficient.

 DATA STRUCTURES FOR CHAINING:
  STRUCTURE        | PROS âœ… / CONS âŒ
  -----------------|-----------------------------------
  Linked List      | âœ… Simple, dynamic insertions & deletions  
                   | âŒ Search time O(n/m) in worst case  
  Balanced BST     | âœ… O(log(n/m)) search  
                   | âœ… Better with high collision load  
                   | âŒ More complex  
  Dynamic Arrays   | âœ… Faster access for search  
                   | âŒ Uses contiguous memory  
                   | âŒ Resizing is costly  
  Hybrid Structures| âœ… Hash table with self-balancing BSTs  

 CONCLUSION:
  -> Chaining effectively handles hash table collisions  
  -> Performance depends on the load factor (Î±)  
  -> Using self-balancing BSTs within chains improves performance for large datasets  
  -> Choosing an optimal hash function and a prime table size minimizes collisions  


2. OPEN ADDRESSING IN HASHING
 -> Open Addressing is a method for handling collisions in hash tables
 -> Unlike chaining, it uses a "SINGLE ARRAY" and does not involve additional data structures
 -> REQUIREMENT: Hash table size (m) >= number of keys (n)
 
 ADVANTAGES
  CACHE-FRIENDLY   -> All elements are in a single array, reducing cache misses
  NO EXTRA MEMORY  -> No need for pointers or linked lists like in chaining

 METHODS OF OPEN ADDRESSING
  I.   LINEAR PROBING
  II.  QUADRATIC PROBING
  III. DOUBLE HASHING
  
 I. LINEAR PROBING
  If a collision occurs, search linearly for the next available slot
  Uses a hash function:  
        hash(key) = key % table_size

  NOTE : If index is occupied, check (index+1) % table_size, and continue

  Example (Table Size = 7)
  | Key | Hash (Key % 7) |          Inserted At            |
  |-----|----------------|---------------------------------|
  | 50  | 50 % 7 = 1     | Index 1                         |
  | 51  | 51 % 7 = 2     | Index 2                         |
  | 49  | 49 % 7 = 0     | Index 0                         |
  | 16  | 16 % 7 = 2     | Collision â†’ Next Slot â†’ Index 3 |
  | 56  | 56 % 7 = 0     | Collision â†’ Next Slot â†’ Index 4 |
  | 15  | 15 % 7 = 1     | Collision â†’ Next Slot â†’ Index 5 |
  | 19  | 19 % 7 = 5     | Collision â†’ Next Slot â†’ Index 6 |

  ==========================================================
  | Index | Key  |
  |-------|------|
  | 0     | 49   |
  | 1     | 50   |
  | 2     | 51   |
  | 3     | 16   |
  | 4     | 56   |
  | 5     | 15   |
  | 6     | 19   |
  ==========================================================

  SEARCHING IN OPEN ADDRESSING
   Compute hash index:  
        hash(key) = key % table_size
  
   If key is not found at computed index:
   1. CONTINUE LINEARLY until key is found or an empty slot is encountered
   2. IF TABLE IS FULL, traverse in a circular manner
   3. STOP IF:
     Key is found â†’ Return `TRUE`
     Empty slot is found â†’ Return `FALSE`
     Entire table is traversed â†’ Return `FALSE`

   ---
  
  DELETION IN OPEN ADDRESSING
   Simply removing an element can BREAK SEARCH OPERATIONS
   SOLUTION  ->  Use a special marker (e.g., 'DELETED' or '-1') instead of making the slot empty
  
  STEPS FOR DELETION:
   1. Search for the key
   2. If found, mark it as `DELETED`.
   3. During search, continue past `DELETED` markers until:
     Key is found (Return `True`)
     Empty slot is found (Return `False`)
     
   ---

  PROBLEMS IN LINEAR PROBING âŒ :
   -> Linear probing resolves collisions by checking the next available slot
   -> Issue: Leads to 'PRIMARY CLUSTERING', where multiple keys crowd around the same index
   Example: 
    `16 % 7 = 2` â†’ Slot 2 is occupied â†’ Moves to Index 3
    `56 % 7 = 0` â†’ Slot 0 is occupied â†’ Moves to Index 4
    `15 % 7 = 1` â†’ Slot 1 is occupied â†’ Moves to Index 5
    `19 % 7 = 5` â†’ Slot 5 is occupied â†’ Moves to Index 6
    PROBLEM: As the table fills up, collisions become frequent, and search times increase.

   ---

 II. QUADRATIC PROBING
 Instead of moving to the next slot, quadratic probing resolves collisions using a quadratic function:
  - **Formula:** `Hash(Key) = (H + iÂ²) % Table_Size`
  - Where `i` starts from `1` and increases (`1Â², 2Â², 3Â², ...`) until an empty slot is found.

#### Example of Quadratic Probing:
  - **Insert `16`** at `16 % 7 = 2` â†’ **Occupied**
  - Check `2 + 1Â² = 3` â†’ **Empty** â†’ Insert at **Index 3**.
  - **Insert `56`** at `56 % 7 = 0` â†’ **Occupied**
  - Check `0 + 1Â² = 1` â†’ **Occupied**
  - Check `0 + 2Â² = 4` â†’ **Empty** â†’ Insert at **Index 4**.

---

### Problems with Quadratic Probing:
1. **Secondary Clustering:**  
   - Quadratic probing reduces **primary clustering**, but **secondary clustering** still occurs.
   - Keys that hash to the same initial index follow the same probe sequence.

2. **Table Size Dependency:**  
   - For quadratic probing to work efficiently, the table size should be **prime**.
   - If the table size is not properly chosen, certain indices might never be reached.

3. **Wasted Space & Unreachable Slots:**  
   - If the table gets too full, quadratic probing might **fail to find an empty slot**.
   - Some indexes may never be checked due to the quadratic growth pattern.

---



## Summary
| Method | Description | Advantages | Disadvantages |
|--------|------------|------------|---------------|
| **Linear Probing** | Search for the next available slot linearly | Simple to implement | Clustering issue (many elements in adjacent slots) |
| **Quadratic Probing** | Search using quadratic increments | Reduces clustering | May not use all slots efficiently |
| **Double Hashing** | Uses a second hash function for jumps | Best distribution of keys | Slightly complex to compute |

âœ” **Best for small datasets**  
âŒ **Chaining is better for large datasets with many collisions**  

---

This structured `.txt` format keeps it clear and easy to read. Let me know if you want any refinements! ğŸš€


